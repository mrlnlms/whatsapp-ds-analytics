"""
Fun√ß√µes de parsing, processamento de m√≠dia e enriquecimento para dados do WhatsApp.

Uso:
    from wrangling import run_wrangling_pipeline, WRANGLING_STEPS
    
    result = run_wrangling_pipeline(
        order=['parse', 'classify', 'media', 'transcriptions', 'enrich', 'export'],
        input_file=Path('data/interim/raw-data_cln7.txt'),
        output_dir=Path('data/processed'),
        media_dir=Path('data/raw/export_2024-10_2025-10/media'),
        transcription_file=Path('transcriptions.csv')
    )
"""

import re
import os
import pandas as pd
from pathlib import Path
from datetime import datetime


# =============================================================================
# FASE 1: PARSING - TXT ‚Üí DataFrame
# =============================================================================

def parse_to_dataframe(input_file: Path) -> pd.DataFrame:
    """
    Converte arquivo TXT limpo em DataFrame estruturado.
    
    Formato esperado: DD/MM/YY HH:MM:SS Remetente: Conte√∫do
    Linhas sem timestamp s√£o agregadas √† mensagem anterior (multilinha).
    
    Args:
        input_file: Path do arquivo TXT limpo
        
    Returns:
        DataFrame com colunas: linha_original, data, hora, timestamp, remetente, conteudo
    """
    message_pattern = r'^(\d{2}/\d{2}/\d{2}) (\d{2}:\d{2}:\d{2}) (.+?): (.*)$'
    
    lines = open(input_file, 'r', encoding='utf-8').readlines()
    
    messages = []
    current_message = None
    
    for line_num, line in enumerate(lines, start=1):
        line = line.rstrip('\n')
        match = re.match(message_pattern, line)
        
        if match:
            # Salva mensagem anterior
            if current_message:
                messages.append(current_message)
            
            # Nova mensagem
            current_message = {
                'linha_original': line_num,
                'data': match.group(1),
                'hora': match.group(2),
                'remetente': match.group(3),
                'conteudo': match.group(4)
            }
        else:
            # Linha de continua√ß√£o
            if current_message:
                current_message['conteudo'] += '\n' + line
    
    # √öltima mensagem
    if current_message:
        messages.append(current_message)
    
    df = pd.DataFrame(messages)
    
    # Cria timestamp datetime
    df['timestamp'] = pd.to_datetime(
        df['data'] + ' ' + df['hora'],
        format='%d/%m/%y %H:%M:%S'
    )
    
    return df


# =============================================================================
# FASE 2: CLASSIFICA√á√ÉO DE MENSAGENS
# =============================================================================

def classify_message_type(content: str) -> str:
    """
    Classifica o tipo de mensagem baseado no conte√∫do.
    
    Tipos:
    - text_pure, text_with_emoji, text_with_link
    - audio_omitted, image_omitted, video_omitted, etc.
    - audio_attached, image_attached, video_attached, etc.
    - message_deleted, message_edited, voice_call, system_message
    """
    content_lower = content.lower().strip()
    
    # Mensagens deletadas/editadas
    if 'this message was deleted' in content_lower:
        return 'message_deleted'
    if '<this message was edited>' in content_lower:
        return 'message_edited'
    
    # Chamadas
    if 'voice call' in content_lower or 'video call' in content_lower:
        if 'missed' in content_lower:
            return 'missed_call'
        return 'voice_call'
    
    # Sistema
    if "this message can't be displayed" in content_lower:
        return 'system_message'
    
    # M√≠dias omitidas
    omitted_types = {
        'audio omitted': 'audio_omitted',
        'image omitted': 'image_omitted',
        'video omitted': 'video_omitted',
        'video note omitted': 'video_note_omitted',
        'sticker omitted': 'sticker_omitted',
        'gif omitted': 'gif_omitted',
        'document omitted': 'document_omitted',
    }
    
    for pattern, msg_type in omitted_types.items():
        if content_lower == pattern or pattern in content_lower:
            return msg_type
    
    # M√≠dias anexadas
    if '<attached:' in content_lower:
        if 'audio' in content_lower or '.opus' in content_lower or '.mp3' in content_lower:
            return 'audio_attached'
        elif 'photo' in content_lower or '.jpg' in content_lower or '.png' in content_lower:
            return 'image_attached'
        elif 'video' in content_lower or '.mp4' in content_lower:
            return 'video_attached'
        elif 'sticker' in content_lower or '.webp' in content_lower:
            return 'sticker_attached'
        elif '.vcf' in content_lower:
            return 'contact_attached'
        else:
            return 'file_attached'
    
    # Texto
    url_pattern = r'https?://[^\s]+'
    if re.search(url_pattern, content):
        return 'text_with_link'
    
    # Emoji (caracteres Unicode > 127, exceto acentos comuns)
    if any(ord(char) > 0x1F600 for char in content):  # Range de emojis
        return 'text_with_emoji'
    
    return 'text_pure'


def add_message_classification(df: pd.DataFrame) -> pd.DataFrame:
    """Adiciona coluna 'tipo_mensagem' ao DataFrame."""
    df = df.copy()
    df['tipo_mensagem'] = df['conteudo'].apply(classify_message_type)
    return df


# =============================================================================
# FASE 3: INVENT√ÅRIO E VINCULA√á√ÉO DE M√çDIA
# =============================================================================

def extract_filename_from_content(content: str) -> str:
    """Extrai nome do arquivo de uma mensagem com m√≠dia anexada."""
    pattern = r'<attached:\s*(.+?)>'
    match = re.search(pattern, content)
    return match.group(1) if match else None


def extract_media_type_from_filename(filename: str) -> str:
    """Extrai tipo de m√≠dia do nome do arquivo (AUDIO, VIDEO, PHOTO, etc.)."""
    if not filename:
        return None
    parts = filename.split('-')
    if len(parts) >= 2:
        return parts[1].upper()
    return None


def inventory_media_files(media_dir: Path) -> pd.DataFrame:
    """
    Lista todos os arquivos de m√≠dia no diret√≥rio.
    
    Args:
        media_dir: Diret√≥rio com arquivos de m√≠dia
        
    Returns:
        DataFrame com: filename, path, extension, size_bytes, media_type
    """
    media_dir = Path(media_dir)
    
    if not media_dir.exists():
        print(f"‚ö†Ô∏è Diret√≥rio n√£o encontrado: {media_dir}")
        return pd.DataFrame()
    
    files = []
    for file_path in media_dir.iterdir():
        if file_path.is_file():
            files.append({
                'filename': file_path.name,
                'path': str(file_path),
                'extension': file_path.suffix.lower(),
                'size_bytes': file_path.stat().st_size,
                'media_type': extract_media_type_from_filename(file_path.name)
            })
    
    return pd.DataFrame(files)


def link_media_to_messages(df: pd.DataFrame, media_dir: Path) -> pd.DataFrame:
    """
    Vincula arquivos de m√≠dia √†s mensagens correspondentes.
    
    Adiciona colunas: arquivo, extensao, tipo_arquivo, arquivo_existe, arquivo_path
    """
    df = df.copy()
    
    # Extrai nome do arquivo de mensagens anexadas
    df['arquivo'] = df['conteudo'].apply(extract_filename_from_content)
    
    # Invent√°rio de arquivos f√≠sicos
    df_inventory = inventory_media_files(media_dir)
    existing_files = set(df_inventory['filename'].values) if not df_inventory.empty else set()
    
    # Verifica exist√™ncia
    df['arquivo_existe'] = df['arquivo'].apply(lambda x: x in existing_files if x else False)
    
    # Extrai metadados
    df['extensao'] = df['arquivo'].apply(lambda x: Path(x).suffix.lower() if x else None)
    df['tipo_arquivo'] = df['arquivo'].apply(extract_media_type_from_filename)
    
    # Path completo
    df['arquivo_path'] = df.apply(
        lambda row: str(media_dir / row['arquivo']) if row['arquivo_existe'] else None,
        axis=1
    )
    
    return df


# =============================================================================
# FASE 4: TRANSCRI√á√ÉO
# =============================================================================

def load_transcriptions(transcription_file: Path) -> pd.DataFrame:
    """
    Carrega arquivo CSV com transcri√ß√µes existentes.
    
    Esperado: file_path, transcription, transcription_status, is_synthetic
    """
    if not transcription_file or not Path(transcription_file).exists():
        print(f"‚ö†Ô∏è Arquivo de transcri√ß√µes n√£o encontrado: {transcription_file}")
        return pd.DataFrame()
    
    df = pd.read_csv(transcription_file)
    
    # Normaliza nome do arquivo (remove path, mant√©m s√≥ filename)
    if 'file_path' in df.columns:
        df['filename'] = df['file_path'].apply(lambda x: Path(x).name if pd.notna(x) else None)
    
    return df


def transcribe_media_groq(file_path: str, api_key: str = None) -> dict:
    """
    Transcreve arquivo de √°udio/v√≠deo usando Groq Whisper API.
    
    Args:
        file_path: Caminho do arquivo
        api_key: Chave da API Groq (ou usa GROQ_API_KEY do ambiente)
        
    Returns:
        Dict com: transcription, status, language, error
        
    NOTA: Requer instala√ß√£o: pip install groq
    """
    try:
        from groq import Groq
    except ImportError:
        return {
            'transcription': None,
            'status': 'error',
            'language': None,
            'error': 'Groq n√£o instalado. Execute: pip install groq'
        }
    
    api_key = api_key or os.getenv('GROQ_API_KEY')
    
    if not api_key:
        return {
            'transcription': None,
            'status': 'error',
            'language': None,
            'error': 'GROQ_API_KEY n√£o configurada'
        }
    
    try:
        client = Groq(api_key=api_key)
        
        with open(file_path, 'rb') as audio_file:
            transcription = client.audio.transcriptions.create(
                file=(Path(file_path).name, audio_file.read()),
                model="whisper-large-v3",
                response_format="verbose_json"
            )
        
        return {
            'transcription': transcription.text,
            'status': 'completed',
            'language': getattr(transcription, 'language', 'unknown'),
            'error': None
        }
        
    except Exception as e:
        return {
            'transcription': None,
            'status': 'error',
            'language': None,
            'error': str(e)
        }


def merge_transcriptions(df: pd.DataFrame, df_transcriptions: pd.DataFrame) -> pd.DataFrame:
    """
    Faz merge das transcri√ß√µes com o DataFrame de mensagens.
    
    Adiciona: tem_transcricao, transcricao, transcription_status, is_synthetic
    """
    df = df.copy()
    
    if df_transcriptions.empty:
        df['tem_transcricao'] = False
        df['transcricao'] = None
        df['transcription_status'] = None
        df['is_synthetic'] = False
        return df
    
    # Cria lookup por filename
    trans_lookup = df_transcriptions.set_index('filename').to_dict('index')
    
    def get_transcription_data(arquivo):
        if not arquivo or arquivo not in trans_lookup:
            return {
                'tem_transcricao': False,
                'transcricao': None,
                'transcription_status': None,
                'is_synthetic': False
            }
        
        data = trans_lookup[arquivo]
        return {
            'tem_transcricao': True,
            'transcricao': data.get('transcription'),
            'transcription_status': data.get('transcription_status'),
            'is_synthetic': data.get('is_synthetic', False)
        }
    
    # Aplica lookup
    trans_data = df['arquivo'].apply(get_transcription_data)
    
    df['tem_transcricao'] = trans_data.apply(lambda x: x['tem_transcricao'])
    df['transcricao'] = trans_data.apply(lambda x: x['transcricao'])
    df['transcription_status'] = trans_data.apply(lambda x: x['transcription_status'])
    df['is_synthetic'] = trans_data.apply(lambda x: x['is_synthetic'])
    
    return df


# =============================================================================
# FASE 5: ENRIQUECIMENTO
# =============================================================================

def enrich_content(df: pd.DataFrame) -> pd.DataFrame:
    """
    Cria coluna 'conteudo_enriquecido' substituindo m√≠dias por transcri√ß√µes.
    
    Para √°udio/v√≠deo com transcri√ß√£o:
        [AUDIO TRANSCRITO] {transcri√ß√£o}
        [Arquivo: filename]
    """
    df = df.copy()
    
    def build_enriched_content(row):
        # Se n√£o tem transcri√ß√£o, mant√©m original
        if not row.get('tem_transcricao') or pd.isna(row.get('transcricao')):
            return row['conteudo']
        
        tipo = row.get('tipo_arquivo', 'MEDIA')
        transcricao = row['transcricao']
        arquivo = row.get('arquivo', '')
        is_synthetic = row.get('is_synthetic', False)
        
        if is_synthetic:
            return f"[{tipo} TRANSCRITO - √ìRF√ÉO] {transcricao}\n[Arquivo: {arquivo}]"
        else:
            return f"[{tipo} TRANSCRITO] {transcricao}\n[Arquivo: {arquivo}]"
    
    df['conteudo_enriquecido'] = df.apply(build_enriched_content, axis=1)
    
    return df


# =============================================================================
# FASE 6: EXPORTA√á√ÉO
# =============================================================================

def export_to_csv(df: pd.DataFrame, output_path: Path, columns: list = None) -> Path:
    """Exporta DataFrame para CSV."""
    output_path = Path(output_path)
    
    if columns:
        df[columns].to_csv(output_path, index=False, encoding='utf-8')
    else:
        df.to_csv(output_path, index=False, encoding='utf-8')
    
    return output_path


def export_corpus_files(df: pd.DataFrame, output_dir: Path, use_enriched: bool = True) -> dict:
    """
    Exporta arquivos TXT de corpus.
    
    Gera:
    - chat_complete.txt: Todas as mensagens
    - chat_p1.txt: S√≥ mensagens de P1
    - chat_p2.txt: S√≥ mensagens de P2
    - corpus_full.txt: Apenas conte√∫do (sem timestamp/remetente)
    
    Args:
        df: DataFrame com mensagens
        output_dir: Diret√≥rio de sa√≠da
        use_enriched: Se True, usa conteudo_enriquecido (com transcri√ß√µes)
        
    Returns:
        Dict com paths dos arquivos gerados
    """
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    content_col = 'conteudo_enriquecido' if use_enriched and 'conteudo_enriquecido' in df.columns else 'conteudo'
    
    files = {}
    
    # Chat completo (com timestamp e remetente)
    chat_complete = output_dir / 'chat_complete.txt'
    with open(chat_complete, 'w', encoding='utf-8') as f:
        for _, row in df.iterrows():
            data = row['timestamp'].strftime('%d/%m/%y')
            hora = row['timestamp'].strftime('%H:%M:%S')
            f.write(f"{data} {hora} {row['remetente']}: {row[content_col]}\n")
    files['chat_complete'] = chat_complete
    
    # Corpus full (s√≥ conte√∫do)
    corpus_full = output_dir / 'corpus_full.txt'
    with open(corpus_full, 'w', encoding='utf-8') as f:
        for conteudo in df[content_col]:
            f.write(f"{conteudo}\n")
    files['corpus_full'] = corpus_full
    
    # Por remetente
    for remetente in df['remetente'].unique():
        df_rem = df[df['remetente'] == remetente]
        
        # Chat com timestamp
        chat_file = output_dir / f'chat_{remetente.lower()}.txt'
        with open(chat_file, 'w', encoding='utf-8') as f:
            for _, row in df_rem.iterrows():
                data = row['timestamp'].strftime('%d/%m/%y')
                hora = row['timestamp'].strftime('%H:%M:%S')
                f.write(f"{data} {hora} {row['remetente']}: {row[content_col]}\n")
        files[f'chat_{remetente.lower()}'] = chat_file
        
        # Corpus s√≥ conte√∫do
        corpus_file = output_dir / f'corpus_{remetente.lower()}.txt'
        with open(corpus_file, 'w', encoding='utf-8') as f:
            for conteudo in df_rem[content_col]:
                f.write(f"{conteudo}\n")
        files[f'corpus_{remetente.lower()}'] = corpus_file
    
    return files


# =============================================================================
# COLUNAS PARA EXPORTA√á√ÉO
# =============================================================================

# Todas as colunas (debug/auditoria)
COLUMNS_FULL = [
    'linha_original',
    'data',
    'hora',
    'timestamp',
    'remetente',
    'conteudo',
    'tipo_mensagem',
    'arquivo',
    'arquivo_existe',
    'extensao',
    'tipo_arquivo',
    'arquivo_path',
    'tem_transcricao',
    'transcricao',
    'transcription_status',
    'is_synthetic',
    'conteudo_enriquecido',
]

# Colunas essenciais para an√°lise
COLUMNS_CORE = [
    'timestamp',
    'remetente',
    'tipo_mensagem',
    'conteudo_enriquecido',
    'arquivo',
    'tem_transcricao',
    'transcricao',
    'is_synthetic',
]

# Colunas m√≠nimas (s√≥ o essencial)
COLUMNS_MINIMAL = [
    'timestamp',
    'remetente',
    'tipo_mensagem',
    'conteudo_enriquecido',
]


def export_optimized(
    df: pd.DataFrame, 
    output_dir: Path, 
    formats: list = None,
    show_progress: bool = True
) -> dict:
    """
    Exporta DataFrame em m√∫ltiplos formatos otimizados.
    
    Args:
        df: DataFrame com mensagens
        output_dir: Diret√≥rio de sa√≠da
        formats: Lista de formatos. Default: ['csv_full', 'csv_core', 'parquet']
                 Op√ß√µes: 'csv_full', 'csv_core', 'csv_minimal', 'parquet', 'parquet_minimal'
        show_progress: Se True, imprime progresso
        
    Returns:
        Dict com paths e tamanhos dos arquivos gerados
        
    Formatos:
        - csv_full: Todas as 17 colunas (debug/auditoria)
        - csv_core: 8 colunas essenciais (an√°lise)
        - csv_minimal: 4 colunas m√≠nimas (lightweight)
        - parquet: 8 colunas com tipos otimizados (performance)
        - parquet_minimal: 4 colunas (m√°xima performance)
        
    Nota:
        Parquet requer pyarrow: pip install pyarrow
    """
    if formats is None:
        formats = ['csv_full', 'csv_core', 'parquet']
    
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    outputs = {}
    
    # Verifica se pyarrow est√° dispon√≠vel
    parquet_available = True
    try:
        import pyarrow
    except ImportError:
        parquet_available = False
        if any('parquet' in f for f in formats):
            print("‚ö†Ô∏è pyarrow n√£o instalado. Parquet ser√° ignorado. Instale com: pip install pyarrow")
    
    if show_progress:
        print("üì¶ Exportando datasets otimizados...\n")
    
    # Prepara DataFrame com tipos otimizados para parquet
    def optimize_dtypes(df_subset):
        df_opt = df_subset.copy()
        
        # Timestamp como datetime
        if 'timestamp' in df_opt.columns and df_opt['timestamp'].dtype == 'object':
            df_opt['timestamp'] = pd.to_datetime(df_opt['timestamp'])
        
        # Booleans
        bool_cols = ['tem_transcricao', 'is_synthetic', 'arquivo_existe']
        for col in bool_cols:
            if col in df_opt.columns:
                df_opt[col] = df_opt[col].astype(bool)
        
        # Categorias (baixa cardinalidade)
        cat_cols = ['remetente', 'tipo_mensagem', 'tipo_arquivo', 'extensao']
        for col in cat_cols:
            if col in df_opt.columns:
                df_opt[col] = df_opt[col].astype('category')
        
        return df_opt
    
    # Filtra colunas existentes
    def filter_columns(columns):
        return [c for c in columns if c in df.columns]
    
    for fmt in formats:
        # Pula parquet se n√£o tiver pyarrow
        if 'parquet' in fmt and not parquet_available:
            continue
            
        if fmt == 'csv_full':
            path = output_dir / 'messages_full.csv'
            cols = filter_columns(COLUMNS_FULL)
            df[cols].to_csv(path, index=False, encoding='utf-8')
            
        elif fmt == 'csv_core':
            path = output_dir / 'messages.csv'
            cols = filter_columns(COLUMNS_CORE)
            df[cols].to_csv(path, index=False, encoding='utf-8')
            
        elif fmt == 'csv_minimal':
            path = output_dir / 'messages_minimal.csv'
            cols = filter_columns(COLUMNS_MINIMAL)
            df[cols].to_csv(path, index=False, encoding='utf-8')
            
        elif fmt == 'parquet':
            path = output_dir / 'messages.parquet'
            cols = filter_columns(COLUMNS_CORE)
            df_opt = optimize_dtypes(df[cols])
            df_opt.to_parquet(path, index=False, engine='pyarrow')
            
        elif fmt == 'parquet_minimal':
            path = output_dir / 'messages_minimal.parquet'
            cols = filter_columns(COLUMNS_MINIMAL)
            df_opt = optimize_dtypes(df[cols])
            df_opt.to_parquet(path, index=False, engine='pyarrow')
        
        else:
            continue
        
        # Calcula tamanho
        size_bytes = path.stat().st_size
        size_mb = size_bytes / (1024 * 1024)
        
        outputs[fmt] = {
            'path': path,
            'size_bytes': size_bytes,
            'size_mb': round(size_mb, 2),
            'columns': len(cols)
        }
        
        if show_progress:
            print(f"   ‚úÖ {path.name:<30} {size_mb:>6.2f} MB ({len(cols)} cols)")
    
    if show_progress:
        print(f"\nüìä Compara√ß√£o de tamanhos:")
        if 'csv_full' in outputs and 'parquet' in outputs:
            ratio = outputs['csv_full']['size_mb'] / outputs['parquet']['size_mb']
            print(f"   ‚Ä¢ CSV Full / Parquet: {ratio:.1f}x maior")
        if 'csv_core' in outputs and 'parquet' in outputs:
            savings = (1 - outputs['parquet']['size_mb'] / outputs['csv_core']['size_mb']) * 100
            print(f"   ‚Ä¢ Economia Parquet vs CSV Core: {savings:.1f}%")
    
    return outputs


def get_export_summary(outputs: dict) -> pd.DataFrame:
    """
    Gera DataFrame com resumo dos arquivos exportados.
    
    Args:
        outputs: Dict retornado por export_optimized()
        
    Returns:
        DataFrame com: formato, arquivo, tamanho_mb, colunas
    """
    rows = []
    for fmt, info in outputs.items():
        rows.append({
            'formato': fmt,
            'arquivo': info['path'].name,
            'tamanho_mb': info['size_mb'],
            'colunas': info['columns']
        })
    
    return pd.DataFrame(rows)


# =============================================================================
# REGISTRO DE ETAPAS DO PIPELINE
# =============================================================================

WRANGLING_STEPS = {
    'parse': {
        'name': 'Parsing TXT ‚Üí DataFrame',
        'description': '''Converte arquivo TXT limpo em DataFrame estruturado.
Agrega linhas de continua√ß√£o (mensagens multilinha) e cria timestamp datetime.'''
    },
    'classify': {
        'name': 'Classifica√ß√£o de mensagens',
        'description': '''Classifica cada mensagem por tipo:
- Texto: puro, com emoji, com link
- M√≠dia omitida: audio, image, video, sticker, gif, document
- M√≠dia anexada: audio, image, video, sticker, contact
- Sistema: deleted, edited, calls'''
    },
    'media': {
        'name': 'Vincula√ß√£o de m√≠dia',
        'description': '''Vincula arquivos f√≠sicos de m√≠dia √†s mensagens.
Verifica exist√™ncia, extrai metadados (extens√£o, tipo, tamanho).'''
    },
    'transcriptions': {
        'name': 'Integra√ß√£o de transcri√ß√µes',
        'description': '''Carrega transcri√ß√µes existentes e faz merge com mensagens.
Suporta transcri√ß√µes via Groq/Whisper API ou CSV pr√©-existente.'''
    },
    'enrich': {
        'name': 'Enriquecimento de conte√∫do',
        'description': '''Substitui conte√∫do de m√≠dias por transcri√ß√µes.
Formato: [AUDIO TRANSCRITO] {transcri√ß√£o}'''
    },
    'export': {
        'name': 'Exporta√ß√£o de arquivos',
        'description': '''Exporta datasets em m√∫ltiplos formatos:

**Datasets estruturados:**
- messages_full.csv (17 cols) ‚Äî Debug/auditoria
- messages.csv (8 cols) ‚Äî An√°lise
- messages.parquet (8 cols) ‚Äî Performance

**Arquivos de corpus (TXT):**
- chat_complete.txt, chat_p1.txt, chat_p2.txt
- corpus_full.txt, corpus_p1.txt, corpus_p2.txt'''
    },
}


# =============================================================================
# EXECUTOR DO PIPELINE
# =============================================================================

def run_wrangling_pipeline(
    order: list,
    input_file: Path,
    output_dir: Path,
    media_dir: Path = None,
    transcription_file: Path = None,
    show_progress: bool = True
) -> dict:
    """
    Executa pipeline de wrangling na ordem especificada.
    
    Args:
        order: Lista de IDs das etapas. 
               Ex: ['parse', 'classify', 'media', 'transcriptions', 'enrich', 'export']
        input_file: Path do arquivo TXT limpo (sa√≠da do cleaning)
        output_dir: Path do diret√≥rio para outputs
        media_dir: Path do diret√≥rio com arquivos de m√≠dia
        transcription_file: Path do CSV com transcri√ß√µes existentes
        show_progress: Se True, imprime progresso
        
    Returns:
        Dict com:
        - df: DataFrame final
        - outputs: Dict com paths dos arquivos gerados
        - stats: Estat√≠sticas do processamento
    """
    # Valida IDs
    invalid = [s for s in order if s not in WRANGLING_STEPS]
    if invalid:
        raise ValueError(f"Etapas inv√°lidas: {invalid}. Dispon√≠veis: {list(WRANGLING_STEPS.keys())}")
    
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    df = None
    outputs = {}
    stats = {}
    
    if show_progress:
        print(f"üîÑ Executando pipeline de wrangling com {len(order)} etapas...\n")
    
    for i, step_id in enumerate(order):
        step = WRANGLING_STEPS[step_id]
        step_num = i + 1
        
        if show_progress:
            print(f"   {step_num}. {step['name']}...", end=' ')
        
        # Executa etapa
        if step_id == 'parse':
            df = parse_to_dataframe(input_file)
            stats['total_linhas_txt'] = sum(1 for _ in open(input_file, 'r', encoding='utf-8'))
            stats['total_mensagens'] = len(df)
            if show_progress:
                print(f"‚úÖ ({stats['total_mensagens']:,} mensagens)")
        
        elif step_id == 'classify':
            df = add_message_classification(df)
            stats['tipos_mensagem'] = df['tipo_mensagem'].value_counts().to_dict()
            if show_progress:
                print(f"‚úÖ ({df['tipo_mensagem'].nunique()} tipos)")
        
        elif step_id == 'media':
            if media_dir:
                df = link_media_to_messages(df, Path(media_dir))
                stats['midias_anexadas'] = df['arquivo'].notna().sum()
                stats['midias_existentes'] = df['arquivo_existe'].sum()
                if show_progress:
                    print(f"‚úÖ ({stats['midias_existentes']:,} arquivos encontrados)")
            else:
                if show_progress:
                    print("‚è≠Ô∏è (sem diret√≥rio de m√≠dia)")
        
        elif step_id == 'transcriptions':
            if transcription_file:
                trans_path = Path(transcription_file)
                
                if trans_path.exists():
                    # Arquivo existe ‚Äî carrega e faz merge
                    df_trans = load_transcriptions(trans_path)
                    df = merge_transcriptions(df, df_trans)
                    stats['com_transcricao'] = df['tem_transcricao'].sum()
                    if show_progress:
                        print(f"‚úÖ ({stats['com_transcricao']:,} transcri√ß√µes)")
                else:
                    # Arquivo configurado mas n√£o existe ‚Äî orienta o usu√°rio
                    if show_progress:
                        print(f"‚ö†Ô∏è Arquivo n√£o encontrado")
                        print(f"\n   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê")
                        print(f"   ‚îÇ  Para transcrever os √°udios/v√≠deos:                 ‚îÇ")
                        print(f"   ‚îÇ  1. Execute: python scripts/transcribe_media.py    ‚îÇ")
                        print(f"   ‚îÇ  2. Aguarde o processamento (~40 min)              ‚îÇ")
                        print(f"   ‚îÇ  3. Rode este notebook novamente                   ‚îÇ")
                        print(f"   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò")
                        print(f"   Esperado em: {trans_path}\n")
                    
                    # Continua sem transcri√ß√µes
                    df['tem_transcricao'] = False
                    df['transcricao'] = None
                    df['transcription_status'] = None
                    df['is_synthetic'] = False
                    stats['com_transcricao'] = 0
            else:
                df['tem_transcricao'] = False
                df['transcricao'] = None
                df['transcription_status'] = None
                df['is_synthetic'] = False
                if show_progress:
                    print("‚è≠Ô∏è (sem arquivo de transcri√ß√µes)")
        
        elif step_id == 'enrich':
            df = enrich_content(df)
            stats['mensagens_enriquecidas'] = (df['conteudo'] != df['conteudo_enriquecido']).sum()
            if show_progress:
                print(f"‚úÖ ({stats['mensagens_enriquecidas']:,} enriquecidas)")
        
        elif step_id == 'export':
            # Datasets otimizados (CSV + Parquet)
            optimized_outputs = export_optimized(
                df, output_dir, 
                formats=['csv_full', 'csv_core', 'parquet'],
                show_progress=False
            )
            for fmt, info in optimized_outputs.items():
                outputs[fmt] = info['path']
            
            # Arquivos de corpus (TXTs)
            corpus_files = export_corpus_files(df, output_dir, use_enriched=True)
            outputs.update(corpus_files)
            
            stats['export_summary'] = optimized_outputs
            
            if show_progress:
                print(f"‚úÖ ({len(outputs)} arquivos)")
    
    if show_progress:
        print(f"\n‚úÖ Pipeline conclu√≠do!")
        print(f"   üìä Total de mensagens: {len(df):,}")
        if 'com_transcricao' in stats:
            print(f"   üéôÔ∏è Com transcri√ß√£o: {stats['com_transcricao']:,}")
        print(f"   üìÅ Arquivos gerados: {len(outputs)}")
        
        # Mostra compara√ß√£o de tamanhos
        if 'export_summary' in stats:
            print(f"\n   üì¶ Tamanhos:")
            for fmt, info in stats['export_summary'].items():
                print(f"      ‚Ä¢ {info['path'].name}: {info['size_mb']:.2f} MB")
    
    return {
        'df': df,
        'outputs': outputs,
        'stats': stats,
        'order': order
    }


def get_available_steps() -> list:
    """Retorna lista de IDs de etapas dispon√≠veis."""
    return list(WRANGLING_STEPS.keys())