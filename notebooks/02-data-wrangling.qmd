---
title: "Data Wrangling"
subtitle: "Parsing, classifica√ß√£o, m√≠dia e enriquecimento"
author: "Marlon L."
date: today
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: true
    theme: cosmo
execute:
  warning: false
  echo: false
---

```{python}
#| label: setup
#| code-fold: false

import sys
import os
from dotenv import load_dotenv
from pathlib import Path
import pandas as pd

# Carrega .env e adiciona src ao path
load_dotenv()
sys.path.insert(0, os.getenv('PROJECT_ROOT') + '/src')

from config import PROJECT_ROOT, PATHS
from wrangling import run_wrangling_pipeline, WRANGLING_STEPS, get_export_summary

print(f"üìÅ Projeto: {PROJECT_ROOT}")
```

# Data Wrangling

Com o arquivo limpo gerado no [Data Cleaning](01-data-cleaning.qmd), esta etapa transforma o TXT em um DataFrame estruturado, vincula arquivos de m√≠dia, integra transcri√ß√µes e exporta os dados enriquecidos.

## Objetivo

Transformar o arquivo TXT limpo em dados estruturados e enriquecidos:

-   ‚úÖ Parsing: TXT ‚Üí DataFrame
-   ‚úÖ Classifica√ß√£o: Identificar tipos de mensagem
-   ‚úÖ M√≠dia: Vincular arquivos f√≠sicos
-   ‚úÖ Transcri√ß√£o: Integrar transcri√ß√µes existentes
-   ‚úÖ Enriquecimento: Substituir m√≠dias por transcri√ß√µes
-   ‚úÖ Exporta√ß√£o: CSV + TXTs de corpus

------------------------------------------------------------------------

## Configura√ß√£o do Pipeline

```{python}
#| label: pipeline-config
#| code-fold: false

# =============================================================================
# üîß CONFIGURA√á√ÉO DO PIPELINE
#
# Etapas dispon√≠veis:
#   - 'parse'          : TXT ‚Üí DataFrame (agregar multilinha)
#   - 'classify'       : Classificar tipo de mensagem
#   - 'media'          : Vincular arquivos f√≠sicos de m√≠dia
#   - 'transcriptions' : Integrar transcri√ß√µes existentes
#   - 'enrich'         : Substituir m√≠dias por transcri√ß√µes
#   - 'export'         : Gerar CSV + TXTs de corpus
# =============================================================================

PIPELINE_ORDER = [
    'parse',
    'classify',
    'media',
    'transcriptions',
    'enrich',
    'export',
]

# =============================================================================
# üìÅ CONFIGURA√á√ÉO DE PATHS
# =============================================================================

# Arquivo de entrada (sa√≠da do cleaning)
INPUT_FILE = PATHS['interim'] / 'raw-data_cln7.txt'

# Diret√≥rio de sa√≠da (j√° inclui DATA_FOLDER via config.py)
OUTPUT_DIR = PATHS['processed']

# Diret√≥rio com arquivos de m√≠dia (j√° definido no config.py)
MEDIA_DIR = PATHS['media']

# Arquivo com transcri√ß√µes existentes
# Gerado por: python scripts/transcribe_media.py
TRANSCRIPTION_FILE = OUTPUT_DIR / 'transcriptions.csv'

print(f"üìÑ Entrada: {INPUT_FILE}")
print(f"üìÇ Sa√≠da: {OUTPUT_DIR}")
print(f"üé¨ M√≠dia: {MEDIA_DIR}")
print(f"üéôÔ∏è Transcri√ß√µes: {TRANSCRIPTION_FILE}")
```

------------------------------------------------------------------------

## Execu√ß√£o do Pipeline

```{python}
#| label: execute-pipeline

# Executa o pipeline
result = run_wrangling_pipeline(
    order=PIPELINE_ORDER,
    input_file=INPUT_FILE,
    output_dir=OUTPUT_DIR,
    media_dir=MEDIA_DIR,
    transcription_file=TRANSCRIPTION_FILE,
    show_progress=True
)

# Extrai resultados
df = result['df']
outputs = result['outputs']
stats = result['stats']
```

------------------------------------------------------------------------

## Pipeline de Transforma√ß√£o

```{python}
#| label: render-pipeline
#| output: asis
#| echo: false

for i, step_id in enumerate(PIPELINE_ORDER):
    step = WRANGLING_STEPS[step_id]
    step_num = i + 1
    
    print(f'''<details style="margin: 0.5em 0; list-style: none; border: 1px solid #d4d4d4; border-radius: 5px; padding: 10px;">
<summary style="font-size: 1.3em; font-weight: 600; color: #343a40; list-style: none;">
‚ñ∏ Etapa {step_num}: {step['name']}
</summary>

**Sobre esta transforma√ß√£o:**

{step['description']}

</details>
''')
```

------------------------------------------------------------------------

## Estat√≠sticas do Processamento

```{python}
#| label: show-stats
#| echo: false

print("üìä Estat√≠sticas do Pipeline:\n")

if 'total_linhas_txt' in stats:
    print(f"   ‚Ä¢ Linhas no TXT: {stats['total_linhas_txt']:,}")
if 'total_mensagens' in stats:
    print(f"   ‚Ä¢ Mensagens parseadas: {stats['total_mensagens']:,}")
    ratio = stats['total_linhas_txt'] / stats['total_mensagens']
    print(f"   ‚Ä¢ Raz√£o linhas/mensagem: {ratio:.2f}")

if 'tipos_mensagem' in stats:
    print(f"\nüìã Tipos de mensagem: {len(stats['tipos_mensagem'])}")

if 'midias_anexadas' in stats:
    print(f"\nüé¨ M√≠dias:")
    print(f"   ‚Ä¢ Anexadas: {stats['midias_anexadas']:,}")
    print(f"   ‚Ä¢ Encontradas: {stats.get('midias_existentes', 0):,}")

if 'com_transcricao' in stats:
    print(f"\nüéôÔ∏è Transcri√ß√µes: {stats['com_transcricao']:,}")

if 'mensagens_enriquecidas' in stats:
    print(f"\n‚ú® Mensagens enriquecidas: {stats['mensagens_enriquecidas']:,}")
```

------------------------------------------------------------------------

## Distribui√ß√£o de Tipos de Mensagem

```{python}
#| label: show-message-types
#| echo: false

if 'tipo_mensagem' in df.columns:
    print("üìä Distribui√ß√£o por tipo de mensagem:\n")
    
    type_counts = df['tipo_mensagem'].value_counts()
    
    for msg_type, count in type_counts.items():
        pct = (count / len(df)) * 100
        print(f"   ‚Ä¢ {msg_type:<25} {count:>6,} ({pct:>5.2f}%)")
```

```{python}
#| label: crosstab-type-sender
#| echo: false

if 'tipo_mensagem' in df.columns and 'remetente' in df.columns:
    print("\nüìä Distribui√ß√£o por tipo e remetente:")
    pd.crosstab(df['tipo_mensagem'], df['remetente'], margins=True)
```

------------------------------------------------------------------------

## Arquivos Gerados

::: panel-tabset
## Datasets

```{python}
#| label: show-datasets
#| echo: false

from wrangling import get_export_summary

if 'export_summary' in stats:
    print("üì¶ Datasets exportados:\n")
    df_summary = get_export_summary(stats['export_summary'])
    display(df_summary.style.hide(axis='index'))
    
    # Compara√ß√£o
    if 'csv_full' in stats['export_summary'] and 'parquet' in stats['export_summary']:
        csv_size = stats['export_summary']['csv_full']['size_mb']
        parquet_size = stats['export_summary']['parquet']['size_mb']
        ratio = csv_size / parquet_size
        savings = (1 - parquet_size / csv_size) * 100
        
        print(f"\nüí° Compara√ß√£o:")
        print(f"   ‚Ä¢ CSV Full: {csv_size:.2f} MB (17 colunas)")
        print(f"   ‚Ä¢ Parquet:  {parquet_size:.2f} MB (8 colunas)")
        print(f"   ‚Ä¢ Economia: {savings:.1f}% ({ratio:.1f}x menor)")
```

## Corpus (TXT)

```{python}
#| label: show-corpus
#| echo: false

print("üìÑ Arquivos de corpus:\n")

corpus_keys = ['chat_complete', 'chat_p1', 'chat_p2', 'corpus_full', 'corpus_p1', 'corpus_p2']

for key in corpus_keys:
    if key in outputs:
        path = outputs[key]
        size = os.path.getsize(path) if os.path.exists(path) else 0
        size_mb = size / (1024 * 1024)
        print(f"   ‚Ä¢ {path.name:<25} {size_mb:>6.2f} MB")
```
:::

------------------------------------------------------------------------

::: {.callout-tip icon="false" collapse="false"}
## üéØ Resultado Final

```{python}
#| echo: false

print(f"üìä DataFrame final: {len(df):,} mensagens √ó {len(df.columns)} colunas")
print(f"üìã Colunas: {', '.join(df.columns[:10])}...")
print(f"\nüìÖ Per√≠odo: {df['timestamp'].min()} at√© {df['timestamp'].max()}")

if 'remetente' in df.columns:
    print(f"\nüë• Participantes:")
    for rem, count in df['remetente'].value_counts().items():
        print(f"   ‚Ä¢ {rem}: {count:,} mensagens ({count/len(df)*100:.1f}%)")

print(f"\n‚úÖ Arquivo principal: {outputs.get('csv', 'N/A')}")
```
:::

------------------------------------------------------------------------

## Preview dos Dados

```{python}
#| label: preview-data
#| echo: false

# Seleciona colunas para preview
preview_cols = ['timestamp', 'remetente', 'tipo_mensagem', 'conteudo']
if 'conteudo_enriquecido' in df.columns:
    preview_cols.append('conteudo_enriquecido')

print("üîç Primeiras mensagens:\n")
df[preview_cols].head(10)
```

```{python}
#| label: preview-transcribed
#| echo: false

if 'tem_transcricao' in df.columns:
    df_trans = df[df['tem_transcricao'] == True]
    
    if len(df_trans) > 0:
        print(f"\nüéôÔ∏è Exemplos de mensagens transcritas ({len(df_trans)} total):\n")
        
        preview_cols = ['timestamp', 'remetente', 'tipo_arquivo', 'conteudo_enriquecido']
        df_trans[preview_cols].head(5)
```

------------------------------------------------------------------------

## üìÅ Gloss√°rio de Arquivos Gerados

Esta se√ß√£o documenta todos os arquivos gerados pelo pipeline, organizados por fun√ß√£o e utilidade.

::: panel-tabset
## üéØ Arquivos Principais

Estes s√£o os arquivos que voc√™ vai usar nas pr√≥ximas etapas (Feature Engineering, An√°lises):

| Arquivo | Formato | Colunas | Uso |
|-------------------|-------------------|-------------------|-----------------|
| `messages.parquet` | Parquet | 8 | **Dataset principal para an√°lise.** Tipos otimizados, leitura r√°pida, menor tamanho. Use este! |
| `messages.csv` | CSV | 8 | Alternativa ao Parquet. √ötil para abrir no Excel ou compartilhar. |
| `chat_complete.txt` | TXT | ‚Äî | Chat completo com transcri√ß√µes. Formato: `DD/MM/YY HH:MM:SS Remetente: Conte√∫do` |

**Colunas do dataset principal (8):**

```         
timestamp, remetente, tipo_mensagem, conteudo_enriquecido,
arquivo, tem_transcricao, transcricao, is_synthetic
```

## üìä Corpus para NLP

Arquivos de texto puro para an√°lises de linguagem natural:

| Arquivo | Descri√ß√£o | Uso |
|-------------------------|------------------------------|-----------------|
| `corpus_full.txt` | Todo o conte√∫do, sem metadados | Treinar modelos, word clouds, an√°lise de vocabul√°rio |
| `corpus_p1.txt` | Apenas mensagens de P1 | An√°lise individual, compara√ß√£o de estilos |
| `corpus_p2.txt` | Apenas mensagens de P2 | An√°lise individual, compara√ß√£o de estilos |
| `chat_p1.txt` | Mensagens de P1 com timestamp | An√°lise temporal por pessoa |
| `chat_p2.txt` | Mensagens de P2 com timestamp | An√°lise temporal por pessoa |

## üîç Debug/Auditoria

Arquivos com informa√ß√µes completas para verifica√ß√£o e debug:

| Arquivo | Colunas | Descri√ß√£o |
|-----------------------|-----------------------|---------------------------|
| `messages_full.csv` | 17 | Todas as colunas, incluindo intermedi√°rias (`linha_original`, `arquivo_path`, etc.) |
| `messages_enriched.csv` | 17 | Legado ‚Äî mesmo conte√∫do que `messages_full.csv` |

**Colunas extras (debug):**

```         
linha_original    ‚Üí Refer√™ncia ao TXT original
data, hora        ‚Üí Redundante (j√° tem timestamp)
arquivo_existe    ‚Üí Bool de verifica√ß√£o
arquivo_path      ‚Üí Path completo do arquivo
extensao          ‚Üí Deriv√°vel de 'arquivo'
tipo_arquivo      ‚Üí Deriv√°vel de 'arquivo'
transcription_status ‚Üí Status da API
```

## üì• Dados de Entrada

Arquivos que alimentaram o pipeline (n√£o s√£o outputs, mas ficam na pasta):

| Arquivo              | Descri√ß√£o                                     |
|----------------------|-----------------------------------------------|
| `transcriptions.csv` | Transcri√ß√µes de √°udio/v√≠deo (input da Fase 4) |
:::

------------------------------------------------------------------------

::: {.callout-tip icon="false"}
## üí° Recomenda√ß√£o de Uso

**Para as pr√≥ximas etapas do projeto, use:**

``` python
# Carrega dataset principal (r√°pido, tipos corretos)
df = pd.read_parquet('data/processed/messages.parquet')

# Ou se preferir CSV
df = pd.read_csv('data/processed/messages.csv')
```

**Para an√°lises de NLP:**

``` python
# Texto completo
with open('data/processed/corpus_full.txt', 'r') as f:
    texto = f.read()

# Por participante
with open('data/processed/corpus_p1.txt', 'r') as f:
    texto_p1 = f.read()
```

**Arquivos que podem ser deletados ap√≥s verifica√ß√£o:**

-   `messages_enriched.csv` (duplicado de `messages_full.csv`)
-   `messages_full.csv` (s√≥ precisa se for debugar)
:::

------------------------------------------------------------------------

## Exporta√ß√£o Manual (Opcional)

::: {.callout-note appearance="simple" collapse="true"}
### üìã Como re-exportar com configura√ß√µes diferentes

``` python
from wrangling import export_corpus_files, export_to_csv

# Re-exportar s√≥ texto (sem tags de m√≠dia)
df_text = df[~df['tipo_mensagem'].str.contains('omitted|attached')]
export_corpus_files(df_text, OUTPUT_DIR / 'text_only', use_enriched=False)

# Exportar CSV com colunas espec√≠ficas
cols = ['timestamp', 'remetente', 'conteudo', 'tipo_mensagem']
export_to_csv(df, OUTPUT_DIR / 'messages_minimal.csv', columns=cols)
```
:::

------------------------------------------------------------------------

# Pr√≥ximos Passos

Com os dados estruturados e enriquecidos, seguimos para:

1.  [**Feature Engineering**](03-feature-engineering.qmd) ‚Äî Cria√ß√£o de vari√°veis derivadas
2.  [**An√°lises Descritivas**](04-descriptive-analysis.qmd) ‚Äî Estat√≠sticas e visualiza√ß√µes